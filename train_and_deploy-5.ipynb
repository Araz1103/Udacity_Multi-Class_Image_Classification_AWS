{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Multi-Class Image Classification on Dogs Breed Dataset, with Amazon Sagemaker \n",
    "\n",
    "## I Fine Tuning a Pre-Trained Model on this dataset\n",
    "## II Model Debugging and Profiling\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting smdebug\n",
      "  Downloading smdebug-1.0.12-py2.py3-none-any.whl (270 kB)\n",
      "     |████████████████████████████████| 270 kB 29.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from smdebug) (20.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (3.19.1)\n",
      "Collecting pyinstrument==3.4.2\n",
      "  Downloading pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
      "     |████████████████████████████████| 83 kB 179 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.10.32 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.23)\n",
      "Collecting pyinstrument-cext>=0.2.2\n",
      "  Downloading pyinstrument_cext-0.2.4-cp37-cp37m-manylinux2010_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (1.23.23)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (1.26.7)\n",
      "Installing collected packages: pyinstrument-cext, pyinstrument, smdebug\n",
      "Successfully installed pyinstrument-3.4.2 pyinstrument-cext-0.2.4 smdebug-1.0.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install any packages that you might need\n",
    "# For instance, you will need the smdebug package\n",
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## We are using the Dogs Breed Multi-Class Classification Dataset\n",
    "\n",
    "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have downloaded the dataset and uploaded it to AWS S3\n",
    "- Uploaded Data to a S3 Bucket\n",
    "- Checked all data is present currently\n",
    "- Understood Distribution of Train-Validation-Test\n",
    "\n",
    "### Dataset Link for Reference\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any packages that you might need\n",
    "# For instance you will need Boto3 and Sagemaker\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking All Data is uploaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-studio-88wth0tideu'\n",
    "subfolder = 'dogImages/train'\n",
    "#subfolder = 'dogImages/valid'\n",
    "#subfolder = 'dogImages/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = conn.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_set = []\n",
    "for f in contents:\n",
    "    #print(f['Key'])\n",
    "    uploaded_set.append(f['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_set = []\n",
    "f = open(\"dog_train.txt\", \"r+\") #Checked for valid and test also\n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    original_set.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_set = set(original_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_set = sorted(original_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_set = set(uploaded_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_difference = original_set - uploaded_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = []\n",
    "f = open(\"dog_train_folders.txt\", \"r+\")\n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    train_folders.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = sorted(train_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "6680\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "files_folder_wise = []\n",
    "for folder in train_folders:\n",
    "    folder_files = []\n",
    "    for f in original_set:\n",
    "        if folder in f:\n",
    "            folder_files.append(f)\n",
    "            i+=1\n",
    "    files_folder_wise.append(folder_files)\n",
    "    \n",
    "print(len(files_folder_wise))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cnt = 0\n",
    "for folder in train_folders:\n",
    "    #print(folder)\n",
    "    sub_f = folder\n",
    "    i+=1\n",
    "    try:\n",
    "        contents = conn.list_objects(Bucket=bucket, Prefix=sub_f)['Contents']\n",
    "    except:\n",
    "        print(\"Folder Doesn't exist\")\n",
    "        print(folder)\n",
    "        f_cnt+=1\n",
    "        continue\n",
    "    uploaded_set = []\n",
    "    for f in contents:\n",
    "        #print(f['Key'])\n",
    "        uploaded_set.append(f['Key'])\n",
    "    uploaded_set = set(uploaded_set)\n",
    "    original_set = set(files_folder_wise[f_cnt])\n",
    "    diff = original_set - uploaded_set\n",
    "    #print(diff) #For verification that all folders are complete\n",
    "    f_cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Distribution Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Classes (Breeds) = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_folder_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogImages/train/001.Affenpinscher',\n",
       " 'dogImages/train/002.Afghan_hound',\n",
       " 'dogImages/train/003.Airedale_terrier',\n",
       " 'dogImages/train/004.Akita',\n",
       " 'dogImages/train/005.Alaskan_malamute',\n",
       " 'dogImages/train/006.American_eskimo_dog',\n",
       " 'dogImages/train/007.American_foxhound',\n",
       " 'dogImages/train/008.American_staffordshire_terrier',\n",
       " 'dogImages/train/009.American_water_spaniel',\n",
       " 'dogImages/train/010.Anatolian_shepherd_dog',\n",
       " 'dogImages/train/011.Australian_cattle_dog',\n",
       " 'dogImages/train/012.Australian_shepherd',\n",
       " 'dogImages/train/013.Australian_terrier',\n",
       " 'dogImages/train/014.Basenji',\n",
       " 'dogImages/train/015.Basset_hound',\n",
       " 'dogImages/train/016.Beagle',\n",
       " 'dogImages/train/017.Bearded_collie',\n",
       " 'dogImages/train/018.Beauceron',\n",
       " 'dogImages/train/019.Bedlington_terrier',\n",
       " 'dogImages/train/020.Belgian_malinois',\n",
       " 'dogImages/train/021.Belgian_sheepdog',\n",
       " 'dogImages/train/022.Belgian_tervuren',\n",
       " 'dogImages/train/023.Bernese_mountain_dog',\n",
       " 'dogImages/train/024.Bichon_frise',\n",
       " 'dogImages/train/025.Black_and_tan_coonhound',\n",
       " 'dogImages/train/026.Black_russian_terrier',\n",
       " 'dogImages/train/027.Bloodhound',\n",
       " 'dogImages/train/028.Bluetick_coonhound',\n",
       " 'dogImages/train/029.Border_collie',\n",
       " 'dogImages/train/030.Border_terrier',\n",
       " 'dogImages/train/031.Borzoi',\n",
       " 'dogImages/train/032.Boston_terrier',\n",
       " 'dogImages/train/033.Bouvier_des_flandres',\n",
       " 'dogImages/train/034.Boxer',\n",
       " 'dogImages/train/035.Boykin_spaniel',\n",
       " 'dogImages/train/036.Briard',\n",
       " 'dogImages/train/037.Brittany',\n",
       " 'dogImages/train/038.Brussels_griffon',\n",
       " 'dogImages/train/039.Bull_terrier',\n",
       " 'dogImages/train/040.Bulldog',\n",
       " 'dogImages/train/041.Bullmastiff',\n",
       " 'dogImages/train/042.Cairn_terrier',\n",
       " 'dogImages/train/043.Canaan_dog',\n",
       " 'dogImages/train/044.Cane_corso',\n",
       " 'dogImages/train/045.Cardigan_welsh_corgi',\n",
       " 'dogImages/train/046.Cavalier_king_charles_spaniel',\n",
       " 'dogImages/train/047.Chesapeake_bay_retriever',\n",
       " 'dogImages/train/048.Chihuahua',\n",
       " 'dogImages/train/049.Chinese_crested',\n",
       " 'dogImages/train/050.Chinese_shar-pei',\n",
       " 'dogImages/train/051.Chow_chow',\n",
       " 'dogImages/train/052.Clumber_spaniel',\n",
       " 'dogImages/train/053.Cocker_spaniel',\n",
       " 'dogImages/train/054.Collie',\n",
       " 'dogImages/train/055.Curly-coated_retriever',\n",
       " 'dogImages/train/056.Dachshund',\n",
       " 'dogImages/train/057.Dalmatian',\n",
       " 'dogImages/train/058.Dandie_dinmont_terrier',\n",
       " 'dogImages/train/059.Doberman_pinscher',\n",
       " 'dogImages/train/060.Dogue_de_bordeaux',\n",
       " 'dogImages/train/061.English_cocker_spaniel',\n",
       " 'dogImages/train/062.English_setter',\n",
       " 'dogImages/train/063.English_springer_spaniel',\n",
       " 'dogImages/train/064.English_toy_spaniel',\n",
       " 'dogImages/train/065.Entlebucher_mountain_dog',\n",
       " 'dogImages/train/066.Field_spaniel',\n",
       " 'dogImages/train/067.Finnish_spitz',\n",
       " 'dogImages/train/068.Flat-coated_retriever',\n",
       " 'dogImages/train/069.French_bulldog',\n",
       " 'dogImages/train/070.German_pinscher',\n",
       " 'dogImages/train/071.German_shepherd_dog',\n",
       " 'dogImages/train/072.German_shorthaired_pointer',\n",
       " 'dogImages/train/073.German_wirehaired_pointer',\n",
       " 'dogImages/train/074.Giant_schnauzer',\n",
       " 'dogImages/train/075.Glen_of_imaal_terrier',\n",
       " 'dogImages/train/076.Golden_retriever',\n",
       " 'dogImages/train/077.Gordon_setter',\n",
       " 'dogImages/train/078.Great_dane',\n",
       " 'dogImages/train/079.Great_pyrenees',\n",
       " 'dogImages/train/080.Greater_swiss_mountain_dog',\n",
       " 'dogImages/train/081.Greyhound',\n",
       " 'dogImages/train/082.Havanese',\n",
       " 'dogImages/train/083.Ibizan_hound',\n",
       " 'dogImages/train/084.Icelandic_sheepdog',\n",
       " 'dogImages/train/085.Irish_red_and_white_setter',\n",
       " 'dogImages/train/086.Irish_setter',\n",
       " 'dogImages/train/087.Irish_terrier',\n",
       " 'dogImages/train/088.Irish_water_spaniel',\n",
       " 'dogImages/train/089.Irish_wolfhound',\n",
       " 'dogImages/train/090.Italian_greyhound',\n",
       " 'dogImages/train/091.Japanese_chin',\n",
       " 'dogImages/train/092.Keeshond',\n",
       " 'dogImages/train/093.Kerry_blue_terrier',\n",
       " 'dogImages/train/094.Komondor',\n",
       " 'dogImages/train/095.Kuvasz',\n",
       " 'dogImages/train/096.Labrador_retriever',\n",
       " 'dogImages/train/097.Lakeland_terrier',\n",
       " 'dogImages/train/098.Leonberger',\n",
       " 'dogImages/train/099.Lhasa_apso',\n",
       " 'dogImages/train/100.Lowchen',\n",
       " 'dogImages/train/101.Maltese',\n",
       " 'dogImages/train/102.Manchester_terrier',\n",
       " 'dogImages/train/103.Mastiff',\n",
       " 'dogImages/train/104.Miniature_schnauzer',\n",
       " 'dogImages/train/105.Neapolitan_mastiff',\n",
       " 'dogImages/train/106.Newfoundland',\n",
       " 'dogImages/train/107.Norfolk_terrier',\n",
       " 'dogImages/train/108.Norwegian_buhund',\n",
       " 'dogImages/train/109.Norwegian_elkhound',\n",
       " 'dogImages/train/110.Norwegian_lundehund',\n",
       " 'dogImages/train/111.Norwich_terrier',\n",
       " 'dogImages/train/112.Nova_scotia_duck_tolling_retriever',\n",
       " 'dogImages/train/113.Old_english_sheepdog',\n",
       " 'dogImages/train/114.Otterhound',\n",
       " 'dogImages/train/115.Papillon',\n",
       " 'dogImages/train/116.Parson_russell_terrier',\n",
       " 'dogImages/train/117.Pekingese',\n",
       " 'dogImages/train/118.Pembroke_welsh_corgi',\n",
       " 'dogImages/train/119.Petit_basset_griffon_vendeen',\n",
       " 'dogImages/train/120.Pharaoh_hound',\n",
       " 'dogImages/train/121.Plott',\n",
       " 'dogImages/train/122.Pointer',\n",
       " 'dogImages/train/123.Pomeranian',\n",
       " 'dogImages/train/124.Poodle',\n",
       " 'dogImages/train/125.Portuguese_water_dog',\n",
       " 'dogImages/train/126.Saint_bernard',\n",
       " 'dogImages/train/127.Silky_terrier',\n",
       " 'dogImages/train/128.Smooth_fox_terrier',\n",
       " 'dogImages/train/129.Tibetan_mastiff',\n",
       " 'dogImages/train/130.Welsh_springer_spaniel',\n",
       " 'dogImages/train/131.Wirehaired_pointing_griffon',\n",
       " 'dogImages/train/132.Xoloitzcuintli',\n",
       " 'dogImages/train/133.Yorkshire_terrier']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folders #Name of each Breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "f = open(\"dog_train.txt\", \"r+\") \n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    train_set.append(line[:-1])\n",
    "    \n",
    "valid_set = []\n",
    "f = open(\"dog_valid.txt\", \"r+\") \n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    valid_set.append(line[:-1])\n",
    "    \n",
    "test_set = []\n",
    "f = open(\"dog_test.txt\", \"r+\") \n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    test_set.append(line[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data = 6680 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6680"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Data = 836 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data = 836 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Data = 8351 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8351"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set) + len(valid_set) + len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare your HP ranges, metrics etc.\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(10,20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create estimators for your HPs\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"hpo.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=1,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")# TODO: Your HP tuner here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"s3://sagemaker-studio-88wth0tideu/dogImages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................*\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for HyperParameterTuning job pytorch-training-220211-2038: Failed. Reason: All training jobs failed. Please take a look at the training jobs failures to get more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-1794cd2f749b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Fit your HP Tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_with_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3253\u001b[0m         \"\"\"\n\u001b[1;32m   3254\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tuning_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperParameterTuningJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3336\u001b[0m                 ),\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m             )\n\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for HyperParameterTuning job pytorch-training-220211-2038: Failed. Reason: All training jobs failed. Please take a look at the training jobs failures to get more details."
     ]
    }
   ],
   "source": [
    "# TODO: Fit your HP Tuner\n",
    "tuner.fit({\"training\": training_path}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the best estimators and the best HPs\n",
    "\n",
    "best_estimator = tuner.best_estimator()#TODO\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up debugging and profiling rules and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator\n",
    "\n",
    "estimator = # TODO: Your estimator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint\n",
    "\n",
    "predictor=estimator.deploy() # TODO: Add your deployment configuration like instance type and number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint\n",
    "\n",
    "image = # TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
    "response = predictor.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
